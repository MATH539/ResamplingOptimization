---
title: "SCCWRP Bight 2013 Zinc"
author: "Lucy Zhang"
date: "2/26/2020"
output: pdf_document
---

Kernel smooth an intensity rate for zinc for Bight 2013 data.

```{r echo = FALSE}
#read in 2013 zinc bight data

zdata = read.csv("/Users/lucyzhang/Desktop/Lucy/Math 539 Statistical Consulting/SCCWRP/Bight 2013 Zinc.csv",h=T)
View(zdata)
zdata1 = zdata[,c(-7,-8)]
View(zdata1)

#omit rows with NA values for X_COORDINATE (latitude = east/west) and Y_COORDINATE (longitude = north/south)
data = data.frame(x = na.omit(zdata1$latitude), y = na.omit(zdata1$longitude))
n = length(data$x)

#convert X_COORDINATE and Y_COORDINATE values to fit [0,1] by [0,1] frame
min.x = min(data$x)
max.x = max(data$x)
min.y = min(data$y)
max.y = max(data$y)
data$x = (data$x - min.x)/(max.x - min.x)
data$y = (data$y - min.y)/(max.y - min.y)

#plot x,y coordinates
plot(data$x, data$y, xlab = 'latitude', ylab = 'longitude', main = 'Bight Zinc 2013 Locations')

#var.x = var(data$x)
#var.y = var(data$y)
#cov.xy = cov(data$x,data$y)
```

```{r echo = FALSE}
###Select our Kernel...Gaussian seems fine...We're going to cross validate 10 separate times. Each with a disjoint partitioning of 10% of the data as test data.  

####STEP 1:  Partition Data into Training and Testing data....

	shuffle.index = sample(1:n,n,replace=F)
	test.index = vector("list",10)
	for(k in 1:10){
		test.index[[k]] = shuffle.index[(1+55*(k-1)):(55*k)] 
	}

###Lets compute the cross validation log-likelihood.  Sum(log(lambda(s_i)) = integral of lambda(s) over S)
###h.vec[1] is sigma.x^2 (variance of x), h.vec[2] is sigma.y^2 (variance of y), h.vec3 = cov(x,y)
	
#10 separate times, I'm going to partition 10% of the data into testing data, 
#use the remaining 90% to estimate the pseudo-loglikelihood for 10% of the data, 
#and add that value to my log-likelihood. Keep updating that over and over and over again. 
#Spit out the log-likelihood at the end of the function.	
	
 ###STEP 2:  Do the kernel density estimation just using the model data

##Gbins has seven columns, identifying the spatial parameter, the spatial area and
##the number of test.data in each bin.
##the 400 bins correspond to 400 .05 by .05 squares.
##We will need Gbins to ultimately compute the Pseudo Log Likelihood for each selected choice
##of h.  

#Column1 = index
#Column2,3,4,5 = firstx,secondx,firsty,secondy
#Column6 = Area of bin = (always = .0025) for this data (|G(j)| in our notes)
#Column7 = Number of Test shots in each bin from test data (known as kj in our notes)
#Column8 = Average Kernel over the bin based on Model.data * bin area. (delta*f^(xj))
#Column9 = Contribution to LogLikelihood (result of PART 4: in our notes)
	
###In order to compute Gbins[,8] we are first going to need to come up with a distance matrix from each bins center to the data points in our model data.  This will be a 400 by n matrix.
	
#Because our Bins are so small I consider a single midpoint approximation for the intergral
#This is the equation for STEP 2 from our notes
	
#Now we compute the Log.Likelihood
###Equation from Step 3....

CVLL = function(h.vec){
    H = matrix(c(h.vec[1],h.vec[3],h.vec[3],h.vec[2]),ncol=2) #covariance matrix
LL=0
for(k in 1:10){
	
	model.n = ceiling(n*.9)
	model.data = data[-test.index[[k]],]
	test.data = data[test.index[[k]],]
   
	Gbins = matrix(0,400,9) 
	Gbins[,1] = 1:400
	Gbins[,2] = rep(seq(0,.95,.05),20)
	Gbins[,3] = Gbins[,2]+.05
	Gbins[,4] = rep(seq(0,.95,.05),rep(20,20))
	Gbins[,5] = Gbins[,4]+.05
	Gbins[,6] = .0025
	for(i in 1:400){
		Gbins[i,7] = length(test.data$x[test.data$x >= Gbins[i,2] &
										   test.data$x < Gbins[i,3] &
										   test.data$y >= Gbins[i,4] &
										   test.data$y < Gbins[i,5]])
	}
	
	Gbins.Distance.x = matrix(0,400,model.n)
    Gbins.Distance.y = matrix(0,400,model.n)
   
	for(i in 1:400){
		for(j in 1:model.n){
			Gbins.Distance.x[i,j] = abs(model.data$x[j] - (Gbins[i,2]+.025))
            Gbins.Distance.y[i,j] = abs(model.data$y[j] - (Gbins[i,4]+.025))
		}
	}

    Gbins.Dist = vector("list",400)
    for(i in 1:400){
        Gbins.Dist[[i]] = matrix(c(Gbins.Distance.x[i,],Gbins.Distance.y[i,]),ncol=2)
    }

	for(i in 1:400){
		Gbins[i,8] = (1/9)*Gbins[i,6]*sum((1/(sqrt(det(H))*2*pi))*exp(-apply((Gbins.Dist[[i]]%*%solve(H)*Gbins.Dist[[i]]),1,sum)/2))
	}
    
	Gbins[,9] = log((Gbins[,8]^Gbins[,7])*exp(-Gbins[,8])/factorial(Gbins[,7]))
	LL = LL + sum(Gbins[,9])
}
print(-LL)
-LL
}

#Using optim to find H 
#h = optim(c(0.0014, 0.001, 0.0000258),CVLL,control=list(maxit=2))
#h
```

a.) What kernel did you select? What bandwidth did you select for your kernel? How did you come to select this kernel?

I selected the Gaussian kernel with a bandwidth of 

$$
H = 
\begin{pmatrix}
\sigma_1^2 & \sigma_{12} \\ 
\sigma_{12} & \sigma_2^2
\end{pmatrix} =
\begin{pmatrix}
0.0014 & 0.0000258 \\ 
0.0000258 & 0.001
\end{pmatrix}
$$

I selected this kernel by using a 10-fold cross validation and evaluating the goodness of fit using the pseudo log-likelihood: 

$$
\sum_{i=1}^g log\bigg(\frac{e^{-\lambda_i}\lambda_i^{k_i}}{k_i!}\bigg) \ \ ,g=400^{th} \ bin
$$

For the pseudo log-likelihood, the LA County crime area coordinates (x,y) were first converted to fit a [0,1] by [0,1] spatial window. (A plot of the crime locations is above.) Next, this square was divided into $g=400$ bins (0.05 by 0.05 squares with area = 0.0025 for each square). For each bin, I needed to find the number of testing data points in the bin $(k_i)$ and the expected average number of points for the bin $(\lambda_i)$. $\lambda_i$ was calculated using a single midpoint approximation for the intergral over each bin. The value of the midpoint was obtained by a kernel density estimation with the initial values for our bandwidth H. The equation used for the kernel density estimation is:

$$
\begin{aligned}
\hat{f}_{x,y} (x,y) &= \frac{1}{9} \sum_{i=1}^n K_i((x,y),(x_i,y_i),H) \ \ \ n = number \ of \ training \ data \ points \\
K_i((x,y),(x_i,y_i),H) &= \frac{1}{2\pi\sqrt{det(H)}}exp\bigg(-\frac{((x,y)-(x_i,y_i))^TH^{-1}((x,y)-(x_i,y_i))}{2}\bigg) \\
\lambda_i &= (area \ of \ i^{th} \ bin) * \hat{f}(midpoint(g_i)) \ \ \ \ \ \ ,g_i = i^{th} \ bin \ for \ i = 1,...,400
\end{aligned} 
$$

For the cross validation, I split the data into 10 equally-sized subsets. Each subset took turns being the testing data. For each turn, the remaining data (the 9 other subsets) served as the training data, which was used to estimate the pseudo log-likelihood for the testing data. Each pseudo log-likelihood for the 10 training/testing datasets was summed up to give the final pseudo log-likelihood for the selected bandwidth. Initial values (a vector of the variance of x, the variance of y, and the covariance of x and y) were inputted into the optim function to find the bandwidth which minimized the pseudo log-likelihood. This optimal bandwidth was used to kernel smooth a background rate for LA County crime data.

b.) Provide a visualization of the background rate with the original data plotted over it.

```{r echo = FALSE}
#kernel smooth a background rate for LA County crime data
h = c(0.0014, 0.001, 0.0000258)
H = matrix(c(h[1],h[3],h[3],h[2]),ncol=2)
Gbins = matrix(0,400,9) #making predictions at each square to make heat map
Gbins[,1] = 1:400
Gbins[,2] = rep(seq(0,.95,.05),20)
Gbins[,3] = Gbins[,2]+.05
Gbins[,4] = rep(seq(0,.95,.05),rep(20,20))
Gbins[,5] = Gbins[,4]+.05
Gbins[,6] = .0025
	for(i in 1:400){
		Gbins[i,7] = length(data$x[data$x >= Gbins[i,2] &
										   data$x < Gbins[i,3] &
										   data$y >= Gbins[i,4] &
										   data$y < Gbins[i,5]])
	}

Gbins.Distance.x = matrix(0,400,n) #using all data points. interested in how far away they are from each square
Gbins.Distance.y = matrix(0,400,n)

for(i in 1:400){
    for(j in 1:n){
        Gbins.Distance.x[i,j] = abs(data$x[j] - (Gbins[i,2]+.025))
        Gbins.Distance.y[i,j] = abs(data$y[j] - (Gbins[i,4]+.025))
    }
}

Gbins.Dist = vector("list",400)
for(i in 1:400){
    Gbins.Dist[[i]] = matrix(c(Gbins.Distance.x[i,],Gbins.Distance.y[i,]),ncol=2)
}

	for(i in 1:400){
		Gbins[i,8] = Gbins[i,6]*sum(1/(sqrt(det(H))*2*pi)*exp(-apply((Gbins.Dist[[i]]%*%solve(H)*Gbins.Dist[[i]]),1,sum)/2))
	}

#LL
Gbins[,9] = log((Gbins[,8]^Gbins[,7])*exp(-Gbins[,8])/factorial(Gbins[,7])) 
LL = sum(Gbins[,9])
LL
#head(Gbins[,8])
#head(Gbins[,9])

zz = matrix(Gbins[,8],20,20,byrow=F) #matrix of our intensities
image(1-zz,zlim = c(min(1-zz),max(1-zz)),col=heat.colors(100),xlab = 'X_COORDINATE', ylab = 'Y_COORDINATE')
points(data$x, data$y)
title(main = 'LA County Crime Data Heat Map')
#heat map literally is our model
```

The areas with highest intensity crime rates are colored red, while areas with lowest intensity crime rates are in light beige or light yellow. The colors for the middle intensity rates range from yellow (low intensity) to orange (medium) to red-orange (high).

c.) Change your bandwidth rate (increase it by a magnitude of 10). Provide the resulting kernel smoothing of the background rate.
d.) Compare the results from b and c. Using this comparison, give a brief description on how the log-likelihood for the two is being computed.

```{r echo = FALSE}
h = c(0.014, 0.01, 0.000258)

H = matrix(c(h[1],h[3],h[3],h[2]),ncol=2)
Gbins = matrix(0,400,9) #making predictions at each square to make heat map
Gbins[,1] = 1:400
Gbins[,2] = rep(seq(0,.95,.05),20)
Gbins[,3] = Gbins[,2]+.05
Gbins[,4] = rep(seq(0,.95,.05),rep(20,20))
Gbins[,5] = Gbins[,4]+.05
Gbins[,6] = .0025
	for(i in 1:400){
		Gbins[i,7] = length(data$x[data$x >= Gbins[i,2] &
										   data$x < Gbins[i,3] &
										   data$y >= Gbins[i,4] &
										   data$y < Gbins[i,5]])
	}

Gbins.Distance.x = matrix(0,400,n) #using all data points. interested in how far away they are from each square
Gbins.Distance.y = matrix(0,400,n)

for(i in 1:400){
    for(j in 1:n){
        Gbins.Distance.x[i,j] = abs(data$x[j] - (Gbins[i,2]+.025))
        Gbins.Distance.y[i,j] = abs(data$y[j] - (Gbins[i,4]+.025))
    }
}

Gbins.Dist = vector("list",400)
for(i in 1:400){
    Gbins.Dist[[i]] = matrix(c(Gbins.Distance.x[i,],Gbins.Distance.y[i,]),ncol=2)
}

	for(i in 1:400){
		Gbins[i,8] = Gbins[i,6]*sum(1/(sqrt(det(H))*2*pi)*exp(-apply((Gbins.Dist[[i]]%*%solve(H)*Gbins.Dist[[i]]),1,sum)/2))
	}

#LL
Gbins[,9] = log((Gbins[,8]^Gbins[,7])*exp(-Gbins[,8])/factorial(Gbins[,7])) 
LL = sum(Gbins[,9])
LL
#head(Gbins[,8])
#head(Gbins[,9])

zz = matrix(Gbins[,8],20,20,byrow=F) #matrix of our intensities
image(1-zz,zlim = c(min(1-zz),max(1-zz)),col=heat.colors(100),xlab = 'X_COORDINATE', ylab = 'Y_COORDINATE')
points(data$x, data$y)
title(main = 'LA County Crime Data Heat Map (w/ Increased Bandwidth)')
```

Increasing the bandwidth rate by a magnitude of 10 results in smoother kernels. There is a larger red area (high intensity) surrounded by orange areas (medium intensity) and then yellow and light yellow areas (low intensity). The yellow and light yellow areas are also larger. The log-likelihood is -345.0225 with the first bandwidth (part b) and -655.5013 with the second bandwidth (part c). Therefore, the kernel density estimation with the first bandwidth has a higher likelihood and is a better estimate of the background rate for LA County crime than that with the second bandwidth. The model with the increased bandwidth estimates crime activity where there were no instances observed in the data. The log-likelihood is computed using the whole dataset with the following equation:

$$
\sum_{i=1}^g log\bigg(\frac{e^{-\lambda_i}\lambda_i^{k_i}}{k_i!}\bigg) \ \ ,g=400^{th} \ bin
$$

$k_i$ is the number of data points in the bin and $\lambda_i$ is the expected average number of points for the bin. $\lambda_i$ is calculated using a single midpoint approximation for the intergral over each bin. The value of the midpoint was obtained by a kernel density estimation with the selected values for our bandwidth H. The equation used for the kernel density estimation is:

$$
\begin{aligned}
\hat{f}_{x,y} (x,y) &= \sum_{i=1}^n K_i((x,y),(x_i,y_i),H) \ \ \ n = number \ of \ data \ points \\
K_i((x,y),(x_i,y_i),H) &= \frac{1}{2\pi\sqrt{det(H)}}exp\bigg(-\frac{((x,y)-(x_i,y_i))^TH^{-1}((x,y)-(x_i,y_i))}{2}\bigg) \\
\lambda_i &= (area \ of \ i^{th} \ bin) * \hat{f}(midpoint(g_i)) \ \ \ \ \ \ ,g_i = i^{th} \ bin \ for \ i = 1,...,400
\end{aligned} 
$$

Larger values of $\lambda_i$ with the increased bandwidth mean higher intensity rates, which lead to smaller log-likelihood values for our dataset.

Problem 2:
Manually create four point process realizations with approximately 100-200 points in each. All four datasets should be contained over the spatial [0,1] x [0,1] window. 

The four datasets should have the following features:
dataset1: A realization resulting from a stationary point process;
dataset2: A realization resulting from an inhomogeneous Poisson point process; 
dataset3: A realization resulting from a homogeneous clustering process;
dataset4: A realization resulting from a homogeneous inhibitive process

a.) Provide plots of the data points in each process realization.
b.) Provide plots of $L(h) − h$ against $h$ for each of the datasets you’ve created.
c.) For our L-function plot for dataset2, why does it appear that there is clustering/inhibition?

```{r echo = FALSE, message = FALSE}
library(splancs)

#dataset1: stationary
n1 = rpois(1,150)
ds1 = data.frame(x=runif(n1,0,1),y=runif(n1,0,1))
plot(ds1$x,ds1$y, xlab = 'x', ylab = 'y', main = 'Stationary Point Process')

#L function
b1 = as.points(ds1$x,ds1$y)
bdry = matrix(c(0,0,1,0,1,1,0,1,0,0),ncol=2,byrow=T) #(0,0) -> (1,0) -> (1,1) -> (0,1) -> (0,0)
s = seq(.001,.3,length=50) #s = values at which you want to compute k for 
k4 = khat(b1,bdry,s) #want k4 estimates for b1 points over boundary bdry at s distances #function is called khat
L4 = sqrt(k4/pi)-s #L(h)-h 
plot(c(0,.3),range(L4),type="n",xlab="lag, h",ylab="L4(h) - h", main = 'Stationary Point Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,rep(0,50),lty=2)

#for L(h)-h
k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet = TRUE) #Kenv.csr is the built-in function for simulated bounds
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
	type="n",xlab="distance",ylab="L4(h) - h", main = 'Stationary Point Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,L4upper,lty=2)
lines(s,L4lower,lty=2)
lines(s,rep(0,50),lty=3)
```

The plots of $L(h) − h$ against $h$ for the stationary point process show $L(h) − h$ (line with stars) is between the simulated confidence bounds and around the zero line for each distance (h) from 0 to 0.3.

```{r echo = FALSE}
#dataset2: inhomogeneous
n2 = round(rpois(1,150)/4)*4
ds2 = data.frame(x = c(runif(n2/4,0,1),runif(n2/4,0,.5),runif(n2/4,0,.25),runif(n2/4,0,.1)),y = c(runif(n2/4,0,1),runif(n2/4,0,.5),runif(n2/4,0,.25),runif(n2/4,0,.1)))
plot(ds2$x,ds2$y,xlab = 'x', ylab = 'y', main = 'Inhomogeneous Poisson Point Process')

#L function
b1 = as.points(ds2$x,ds2$y)
bdry = matrix(c(0,0,1,0,1,1,0,1,0,0),ncol=2,byrow=T) #(0,0) -> (1,0) -> (1,1) -> (0,1) -> (0,0)
s = seq(.001,.3,length=50) #s = values at which you want to compute k for 
k4 = khat(b1,bdry,s) #want k4 estimates for b1 points over boundary bdry at s distances #function is called khat
L4 = sqrt(k4/pi)-s #L(h)-h #shows clustering (above zero line)
plot(c(0,.3),range(L4),type="n",xlab="lag, h",ylab="L4(h) - h", main = 'Inhomogeneous Poisson Point Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,rep(0,50),lty=2)

#for L(h)-h
k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet = TRUE)
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
	type="n",xlab="distance",ylab="L4(h) - h", main = 'Inhomogeneous Poisson Point Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,L4upper,lty=2)
lines(s,L4lower,lty=2)
lines(s,rep(0,50),lty=3)
```

The plots of $L(h) − h$ against $h$ for the inhomogeneous Poisson point process show $L(h) − h$ (line with stars) is above the simulated confidence bounds for distances (h) from 0 to 0.3. This indicates clustering. It appears that there is clustering because some areas have higher intensity rates than others. 

```{r echo = FALSE}
#dataset3: homogeneous clustering
#lambda = average number of offspring for each parent point
clust = function(n.cluster=5,lambda=3,xmin=0,xmax=10,ymin=0,ymax=10,norm.sd=.5,plot=T){
  x.main = runif(n.cluster,xmin,xmax)
  y.main = runif(n.cluster,ymin,ymax)
  x.all = x.main
  y.all = y.main
  n.points = rpois(n.cluster,lambda)
  for(i in 1:n.cluster){
    if(n.points[i] > 0){
      for(j in 1:n.points[i]){
        x.points = cumsum(rnorm(n.points[i],0,norm.sd)) + x.main[i]
        y.points = cumsum(rnorm(n.points[i],0,norm.sd)) + y.main[i]	
      }
    }
    else{
      x.points = c()
      y.points = c()
    }
    x.all = c(x.all,x.points)
    y.all = c(y.all,y.points)
  }
  x.added = x.all[(n.cluster+1):length(x.all)]
  y.added = y.all[(n.cluster+1):length(y.all)]
  plot(x.main,y.main,pch='X',cex=1.1,xlim=c(min(x.all)-.1*xmin,max(x.all)+.1*xmin),ylim=c(min(y.all)-.1*ymin,max(y.all)+.1*ymax), xlab = 'x', ylab = 'y', main = 'Homogeneous Clustering Process')
  points(x.added,y.added)
  ds3 = data.frame(x=x.added,y=y.added)
  ds3
}


ds3=clust(n.cluster=10,lambda=15,xmin=0,xmax=1,ymin=0,ymax=1,norm.sd=.03)

#L function
b1 = as.points(ds3$x,ds3$y)
bdry = matrix(c(0,0,1,0,1,1,0,1,0,0),ncol=2,byrow=T) #(0,0) -> (1,0) -> (1,1) -> (0,1) -> (0,0)
s = seq(.001,.3,length=50) #s = values at which you want to compute k for 
k4 = khat(b1,bdry,s) #want k4 estimates for b1 points over boundary bdry at s distances #function is called khat
L4 = sqrt(k4/pi)-s #L(h)-h #shows clustering (above zero line)
plot(c(0,.3),range(L4),type="n",xlab="lag, h",ylab="L4(h) - h", main = 'Homogeneous Clustering Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,rep(0,50),lty=2)

#for L(h)-h
k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet = TRUE)
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
	type="n",xlab="distance",ylab="L4(h) - h", main = 'Homogeneous Clustering Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,L4upper,lty=2)
lines(s,L4lower,lty=2)
lines(s,rep(0,50),lty=3)
```

In the plot of the homogeneous clustering process, the X's represent the parent points for the clusters. The plots of $L(h) − h$ against $h$ show $L(h) − h$ (line with stars) is above the simulated confidence bounds for distances (h) from 0 to 0.3. This indicates clustering.

```{r echo = FALSE}
#dataset4: homogeneous inhibitive
set.seed(1)
n4 = rpois(1,30)
x.temp = rep(0,n4)
y.temp = rep(0,n4)
x.temp[1] = runif(1,0,1)
y.temp[1] = runif(1,0,1)

keeper = function(i,r,p){
  new.x = runif(1,0,1)
  new.y = runif(1,0,1)
  dist = min(sqrt((new.x-x.temp[1:(i-1)])^2 + (new.y-y.temp[1:(i-1)])^2))
  bern = sample(c(0,1),1,prob=c(p,1-p))
  if(dist>r | bern == 1){
    new.x = new.x
    new.y = new.y
    c(new.x,new.y)
  } else{
    new.x = keeper(i,r,p)[1]
    new.y = keeper(i,r,p)[2]
    c(new.x,new.y)
  }
}
 
for(i in 2:n4){
  x.temp[i] = keeper(i,.06,.9)[1]
  y.temp[i] = keeper(i,.06,.9)[2]
}

ds4 = data.frame(x=x.temp,y=y.temp)
plot(ds4$x,ds4$y, xlab = 'x', ylab = 'y', main = 'Homogeneous Inhibitive Process')

#L function
b1 = as.points(ds4$x,ds4$y)
bdry = matrix(c(0,0,1,0,1,1,0,1,0,0),ncol=2,byrow=T) #(0,0) -> (1,0) -> (1,1) -> (0,1) -> (0,0)
s = seq(.001,.3,length=50) #s = values at which you want to compute k for 
k4 = khat(b1,bdry,s) #want k4 estimates for b1 points over boundary bdry at s distances #function is called khat
L4 = sqrt(k4/pi)-s #L(h)-h 
plot(c(0,.3),range(L4),type="n",xlab="lag, h",ylab="L4(h) - h", main = 'Homogeneous Inhibitive Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,rep(0,50),lty=2)

#for L(h)-h
k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet = TRUE)
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
	type="n",xlab="distance",ylab="L4(h) - h", main = 'Homogeneous Inhibitive Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,L4upper,lty=2)
lines(s,L4lower,lty=2)
lines(s,rep(0,50),lty=3)
```

The plots of $L(h) − h$ against $h$ for the homogeneous inhibitive process show $L(h) − h$ (line with stars) is under the zero line for the majority of distances from 0 to 0.3. This indicates inhibition.

##Code Appendix
```{r eval = FALSE}
#read in crime data
LAdata = read.csv("/Users/lucyzhang/Downloads/LA County Crime Data.csv",h=T)

#omit rows with NA values for X_COORDINATE and Y_COORDINATE
data = data.frame(x = na.omit(LAdata$X_COORDINATE), y = na.omit(LAdata$Y_COORDINATE))
n = length(data$x)

#convert X_COORDINATE and Y_COORDINATE values to fit [0,1] by [0,1] frame
min.x = min(data$x)
max.x = max(data$x)
min.y = min(data$y)
max.y = max(data$y)
data$x = (data$x - min.x)/(max.x - min.x)
data$y = (data$y - min.y)/(max.y - min.y)

#plot x,y coordinates
plot(data$x, data$y, xlab = 'X_COORDINATE', ylab = 'Y_COORDINATE', 
     main = 'LA County Crime Locations')

#var.x = var(data$x)
#var.y = var(data$y)
#cov.xy = cov(data$x,data$y)

####STEP 1:  Partition Data into Training and Testing data for CV

	shuffle.index = sample(1:n,n,replace=F)
	test.index = vector("list",10)
	for(k in 1:10){
		test.index[[k]] = shuffle.index[(1+55*(k-1)):(55*k)] 
	}

CVLL = function(h.vec){
    H = matrix(c(h.vec[1],h.vec[3],h.vec[3],h.vec[2]),ncol=2) #covariance matrix
LL=0
for(k in 1:10){
	
	model.n = ceiling(n*.9)
	model.data = data[-test.index[[k]],]
	test.data = data[test.index[[k]],]
   
	Gbins = matrix(0,400,9) 
	Gbins[,1] = 1:400
	Gbins[,2] = rep(seq(0,.95,.05),20)
	Gbins[,3] = Gbins[,2]+.05
	Gbins[,4] = rep(seq(0,.95,.05),rep(20,20))
	Gbins[,5] = Gbins[,4]+.05
	Gbins[,6] = .0025
	for(i in 1:400){
		Gbins[i,7] = length(test.data$x[test.data$x >= Gbins[i,2] &
										   test.data$x < Gbins[i,3] &
										   test.data$y >= Gbins[i,4] &
										   test.data$y < Gbins[i,5]])
	}
	
	Gbins.Distance.x = matrix(0,400,model.n)
    Gbins.Distance.y = matrix(0,400,model.n)
   
	for(i in 1:400){
		for(j in 1:model.n){
			Gbins.Distance.x[i,j] = abs(model.data$x[j] - (Gbins[i,2]+.025))
            Gbins.Distance.y[i,j] = abs(model.data$y[j] - (Gbins[i,4]+.025))
		}
	}

    Gbins.Dist = vector("list",400)
    for(i in 1:400){
        Gbins.Dist[[i]] = matrix(c(Gbins.Distance.x[i,],Gbins.Distance.y[i,]),ncol=2)
    }

	for(i in 1:400){
		Gbins[i,8] = (1/9)*Gbins[i,6]*sum((1/(sqrt(det(H))*2*pi))*
		               exp(-apply((Gbins.Dist[[i]]%*%solve(H)*Gbins.Dist[[i]]),1,sum)/2))
	}
    
	Gbins[,9] = log((Gbins[,8]^Gbins[,7])*exp(-Gbins[,8])/factorial(Gbins[,7]))
	LL = LL + sum(Gbins[,9])
}
print(-LL)
-LL
}

#Using optim to find H 
#h = optim(c(0.0014, 0.001, 0.0000258),CVLL,control=list(maxit=2))
#h

#part b
#kernel smooth a background rate for LA County crime data
h = c(0.0014, 0.001, 0.0000258)
H = matrix(c(h[1],h[3],h[3],h[2]),ncol=2)
Gbins = matrix(0,400,9) #making predictions at each square to make heat map
Gbins[,1] = 1:400
Gbins[,2] = rep(seq(0,.95,.05),20)
Gbins[,3] = Gbins[,2]+.05
Gbins[,4] = rep(seq(0,.95,.05),rep(20,20))
Gbins[,5] = Gbins[,4]+.05
Gbins[,6] = .0025
	for(i in 1:400){
		Gbins[i,7] = length(data$x[data$x >= Gbins[i,2] &
										   data$x < Gbins[i,3] &
										   data$y >= Gbins[i,4] &
										   data$y < Gbins[i,5]])
	}

Gbins.Distance.x = matrix(0,400,n) #using all data points
Gbins.Distance.y = matrix(0,400,n)

for(i in 1:400){
    for(j in 1:n){
        Gbins.Distance.x[i,j] = abs(data$x[j] - (Gbins[i,2]+.025))
        Gbins.Distance.y[i,j] = abs(data$y[j] - (Gbins[i,4]+.025))
    }
}

Gbins.Dist = vector("list",400)
for(i in 1:400){
    Gbins.Dist[[i]] = matrix(c(Gbins.Distance.x[i,],Gbins.Distance.y[i,]),ncol=2)
}

	for(i in 1:400){
		Gbins[i,8] = Gbins[i,6]*sum(1/(sqrt(det(H))*2*pi)*
		            exp(-apply((Gbins.Dist[[i]]%*%solve(H)*Gbins.Dist[[i]]),1,sum)/2))
	}

#LL
Gbins[,9] = log((Gbins[,8]^Gbins[,7])*exp(-Gbins[,8])/factorial(Gbins[,7])) 
LL = sum(Gbins[,9])
LL
#head(Gbins[,8])
#head(Gbins[,9])

zz = matrix(Gbins[,8],20,20,byrow=F) #matrix of our intensities
image(1-zz,zlim = c(min(1-zz),max(1-zz)),col=heat.colors(100),
      xlab = 'X_COORDINATE', ylab = 'Y_COORDINATE')
points(data$x, data$y)
title(main = 'LA County Crime Data Heat Map')
#heat map literally is our model

#part c: increase bandwidth by 10
h = c(0.014, 0.01, 0.000258)

H = matrix(c(h[1],h[3],h[3],h[2]),ncol=2)
Gbins = matrix(0,400,9) #making predictions at each square to make heat map
Gbins[,1] = 1:400
Gbins[,2] = rep(seq(0,.95,.05),20)
Gbins[,3] = Gbins[,2]+.05
Gbins[,4] = rep(seq(0,.95,.05),rep(20,20))
Gbins[,5] = Gbins[,4]+.05
Gbins[,6] = .0025
	for(i in 1:400){
		Gbins[i,7] = length(data$x[data$x >= Gbins[i,2] &
										   data$x < Gbins[i,3] &
										   data$y >= Gbins[i,4] &
										   data$y < Gbins[i,5]])
	}

Gbins.Distance.x = matrix(0,400,n) #using all data points. 
Gbins.Distance.y = matrix(0,400,n)

for(i in 1:400){
    for(j in 1:n){
        Gbins.Distance.x[i,j] = abs(data$x[j] - (Gbins[i,2]+.025))
        Gbins.Distance.y[i,j] = abs(data$y[j] - (Gbins[i,4]+.025))
    }
}

Gbins.Dist = vector("list",400)
for(i in 1:400){
    Gbins.Dist[[i]] = matrix(c(Gbins.Distance.x[i,],Gbins.Distance.y[i,]),ncol=2)
}

	for(i in 1:400){
		Gbins[i,8] = Gbins[i,6]*sum(1/(sqrt(det(H))*2*pi)*
		            exp(-apply((Gbins.Dist[[i]]%*%solve(H)*Gbins.Dist[[i]]),1,sum)/2))
	}

#LL
Gbins[,9] = log((Gbins[,8]^Gbins[,7])*exp(-Gbins[,8])/factorial(Gbins[,7])) 
LL = sum(Gbins[,9])
LL
#head(Gbins[,8])
#head(Gbins[,9])

zz = matrix(Gbins[,8],20,20,byrow=F) #matrix of our intensities
image(1-zz,zlim = c(min(1-zz),max(1-zz)),col=heat.colors(100))
points(data$x, data$y, xlab = 'X_COORDINATE', ylab = 'Y_COORDINATE', 
       main = 'LA County Crime Locations')

#Question 2
library(splancs)

#dataset1: stationary
n1 = rpois(1,150)
ds1 = data.frame(x=runif(n1,0,1),y=runif(n1,0,1))
plot(ds1$x,ds1$y, xlab = 'x', ylab = 'y', main = 'Stationary Point Process')

#L function
b1 = as.points(ds1$x,ds1$y)
bdry = matrix(c(0,0,1,0,1,1,0,1,0,0),ncol=2,byrow=T) 
s = seq(.001,.3,length=50) #s = values at which you want to compute k for 
k4 = khat(b1,bdry,s) #want k4 estimates for b1 points over boundary bdry at s distances
L4 = sqrt(k4/pi)-s #L(h)-h 
plot(c(0,.3),range(L4),type="n",xlab="lag, h",ylab="L4(h) - h", 
     main = 'Stationary Point Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,rep(0,50),lty=2)

#for L(h)-h
k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet = TRUE) #for simulated bounds
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
	type="n",xlab="distance",ylab="L4(h) - h", main = 'Stationary Point Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,L4upper,lty=2)
lines(s,L4lower,lty=2)
lines(s,rep(0,50),lty=3)

#dataset2: inhomogeneous
n2 = round(rpois(1,150)/4)*4
ds2 = data.frame(x = c(runif(n2/4,0,1),runif(n2/4,0,.5),runif(n2/4,0,.25),
      runif(n2/4,0,.1)),y = c(runif(n2/4,0,1),runif(n2/4,0,.5),
                              runif(n2/4,0,.25),runif(n2/4,0,.1)))
plot(ds2$x,ds2$y,xlab = 'x', ylab = 'y', main = 'Inhomogeneous Poisson Point Process')

#L function
b1 = as.points(ds2$x,ds2$y)
bdry = matrix(c(0,0,1,0,1,1,0,1,0,0),ncol=2,byrow=T) 
s = seq(.001,.3,length=50) #s = values at which you want to compute k for 
k4 = khat(b1,bdry,s) #want k4 estimates for b1 points over boundary bdry at s distances
L4 = sqrt(k4/pi)-s #L(h)-h #shows clustering (above zero line)
plot(c(0,.3),range(L4),type="n",xlab="lag, h",ylab="L4(h) - h",
     main = 'Inhomogeneous Poisson Point Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,rep(0,50),lty=2)

#for L(h)-h
k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet = TRUE)
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
	type="n",xlab="distance",ylab="L4(h) - h", 
	main = 'Inhomogeneous Poisson Point Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,L4upper,lty=2)
lines(s,L4lower,lty=2)
lines(s,rep(0,50),lty=3)

#dataset3: homogeneous clustering
#lambda = average number of offspring for each parent point
clust = function(n.cluster=5,lambda=3,xmin=0,xmax=10,ymin=0,ymax=10,norm.sd=.5,plot=T){
  x.main = runif(n.cluster,xmin,xmax)
  y.main = runif(n.cluster,ymin,ymax)
  x.all = x.main
  y.all = y.main
  n.points = rpois(n.cluster,lambda)
  for(i in 1:n.cluster){
    if(n.points[i] > 0){
      for(j in 1:n.points[i]){
        x.points = cumsum(rnorm(n.points[i],0,norm.sd)) + x.main[i]
        y.points = cumsum(rnorm(n.points[i],0,norm.sd)) + y.main[i]	
      }
    }
    else{
      x.points = c()
      y.points = c()
    }
    x.all = c(x.all,x.points)
    y.all = c(y.all,y.points)
  }
  x.added = x.all[(n.cluster+1):length(x.all)]
  y.added = y.all[(n.cluster+1):length(y.all)]
  plot(x.main,y.main,pch='X',cex=1.1,xlim=c(min(x.all)-.1*xmin,max(x.all)+.1*xmin),
       ylim=c(min(y.all)-.1*ymin,max(y.all)+.1*ymax), xlab = 'x', ylab = 'y', 
       main = 'Homogeneous Clustering Process')
  points(x.added,y.added)
  ds3 = data.frame(x=x.added,y=y.added)
  ds3
}


ds3=clust(n.cluster=10,lambda=15,xmin=0,xmax=1,ymin=0,ymax=1,norm.sd=.025)

#L function
b1 = as.points(ds3$x,ds3$y)
bdry = matrix(c(0,0,1,0,1,1,0,1,0,0),ncol=2,byrow=T) 
s = seq(.001,.3,length=50) #s = values at which you want to compute k for 
k4 = khat(b1,bdry,s) #want k4 estimates for b1 points over boundary bdry at s distances 
L4 = sqrt(k4/pi)-s #L(h)-h #shows clustering (above zero line)
plot(c(0,.3),range(L4),type="n",xlab="lag, h",ylab="L4(h) - h", 
     main = 'Homogeneous Clustering Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,rep(0,50),lty=2)

#for L(h)-h
k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet = TRUE)
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
	type="n",xlab="distance",ylab="L4(h) - h", main = 'Homogeneous Clustering Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,L4upper,lty=2)
lines(s,L4lower,lty=2)
lines(s,rep(0,50),lty=3)

#dataset4: homogeneous inhibitive
set.seed(1)
n4 = rpois(1,30)
x.temp = rep(0,n4)
y.temp = rep(0,n4)
x.temp[1] = runif(1,0,1)
y.temp[1] = runif(1,0,1)

keeper = function(i,r,p){
  new.x = runif(1,0,1)
  new.y = runif(1,0,1)
  dist = min(sqrt((new.x-x.temp[1:(i-1)])^2 + (new.y-y.temp[1:(i-1)])^2))
  bern = sample(c(0,1),1,prob=c(p,1-p))
  if(dist>r | bern == 1){
    new.x = new.x
    new.y = new.y
    c(new.x,new.y)
  } else{
    new.x = keeper(i,r,p)[1]
    new.y = keeper(i,r,p)[2]
    c(new.x,new.y)
  }
}
 
for(i in 2:n4){
  x.temp[i] = keeper(i,.06,.9)[1]
  y.temp[i] = keeper(i,.06,.9)[2]
}

ds4 = data.frame(x=x.temp,y=y.temp)
plot(ds4$x,ds4$y, xlab = 'x', ylab = 'y', main = 'Homogeneous Inhibitive Process')

#L function
b1 = as.points(ds4$x,ds4$y)
bdry = matrix(c(0,0,1,0,1,1,0,1,0,0),ncol=2,byrow=T) 
s = seq(.001,.3,length=50) #s = values at which you want to compute k for 
k4 = khat(b1,bdry,s) #want k4 estimates for b1 points over boundary bdry at s distances 
L4 = sqrt(k4/pi)-s #L(h)-h 
plot(c(0,.3),range(L4),type="n",xlab="lag, h",ylab="L4(h) - h", 
     main = 'Homogeneous Inhibitive Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,rep(0,50),lty=2)

#for L(h)-h
k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet = TRUE)
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
	type="n",xlab="distance",ylab="L4(h) - h", main = 'Homogeneous Inhibitive Process')
points(s,L4,pch="*")
lines(s,L4)
lines(s,L4upper,lty=2)
lines(s,L4lower,lty=2)
lines(s,rep(0,50),lty=3)
```